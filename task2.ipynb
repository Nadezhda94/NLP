{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1387,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ds = pd.read_csv(\"hillary-clinton-emails/Emails.csv\", \n",
    "                 delimiter=\",\", usecols =[\"MetadataSubject\",\"ExtractedSubject\", \"ExtractedBodyText\",\"RawText\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Предобработка датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "В качестве текста исходных писем выбиралось поле ExtractedBodyText. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3816,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"H <hrod17@clintonernaii.com›\\nWednesday, September 12, 2012 11:26 PM\\nesullivanjj@state.gov'\\nFw: Fwd: more on libya\\nLibya 37 sept 12 12,docx\\nWe should get this around asap.\""
      ]
     },
     "execution_count": 3816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.ExtractedBodyText[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "На основе \"плохих\" частотных слов и структуры исходных писем выделила небольшой список стоп-слов и выражений, которые надо удалить из исходного текста перед обработкой. Например, строки с датой и пр.метаинформация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3979,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stop_words = [\"UNCLASSIFIED\", \"U.S. Department of State\",\"hillary\"]\n",
    "stop_lines = [\"case no\", \"doc no\", \"importance\",\"cc:\", \"Cc\",\"Sent from\", \"h <hrod17@clintonemail.com>\", \n",
    "              \"sunday\", \"monday\",\"tuesday\", \"wednesday\", \"thursday\",\"friday\", \"saturday\"]\n",
    "stop_phrases = [\"@state.gov\",\"@clintonemail\",\"©state.gov\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Удаляю стоп-слова из писем. Слишком короткие письма не включаю в выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3980,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mails = []\n",
    "titles = []\n",
    "for i in range(1, len(ds.index)):\n",
    "    cur_mail = \"\"\n",
    "  \n",
    "    if type(ds.ExtractedBodyText[i]) is str and len(ds.ExtractedBodyText[i]) > 0:\n",
    "        lines = ds.ExtractedBodyText[i].split(\"\\n\")\n",
    "        cur_mail = \"\"\n",
    "        for line in lines:\n",
    "            for sl in stop_lines:\n",
    "                if line.lower().startswith(sl):\n",
    "                    line = \"\"\n",
    "                    break\n",
    "            for sp in stop_phrases:\n",
    "                if line.find(sp) > -1:\n",
    "                    line = \"\"\n",
    "                    break\n",
    "            if re.search('.docx', line) != None:\n",
    "                line = \"\"\n",
    "                \n",
    "            if len(line) != 0:\n",
    "                cur_word = line.replace(\"\\n\", \" \")\n",
    "                for sw in stop_words:\n",
    "                    cur_word = cur_word.replace(sw, \"\")\n",
    "                cur_mail += \" \" + cur_word + \".\"\n",
    "    if len(cur_mail) > 20:\n",
    "        mails.append(cur_mail)\n",
    "    if type(ds.ExtractedSubject[i]) is str and len(ds.ExtractedSubject[i]) > 3:\n",
    "        titles.append(ds.ExtractedSubject[i].replace(\"\\n\", \" \") +\". \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "source": [
    "### Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "На этом этапе удаляю слишком короткие слова(скорее всего они большой смысловой нагрузки не несут) + использую стеммер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3981,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3982,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemToWord = {}\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if len(token)>3 and re.match('[A-Za-z]+', token) != None:\n",
    "            filtered_tokens.append(token.strip(\".\"))\n",
    "    stems = []\n",
    "    tags = nltk.pos_tag(filtered_tokens)\n",
    "    for token, tag in tags:\n",
    "        stem = stemmer.stem(token)\n",
    "        if len(stem)>3 and (tag[0] == \"N\" or tag[0]==\"V\"):\n",
    "            stems.append(stem)\n",
    "            stemToWord[stem]=token\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3983,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['latest', 'syria', 'qaddafi', 'march', 'hillari']"
      ]
     },
     "execution_count": 3983,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_stem(mails[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3984,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "def filterFrequentWords(n, percent, mails):\n",
    "    vectorizer = CountVectorizer(ngram_range=(n,n),tokenizer=tokenize_and_stem, stop_words=stopwords.words(\"english\"))\n",
    "    X = vectorizer.fit_transform(mails)\n",
    "    \n",
    "    ngramms = defaultdict(int)\n",
    "    END_OF_SENTENCE = \"<END>\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    count = [len(np.nonzero(X[0,:])[1]) ]\n",
    "    wordsCount = 0\n",
    "    for i in X.nonzero()[1]:\n",
    "        ngramms[feature_names[i]] += 1\n",
    "        wordsCount+=1\n",
    "    \n",
    "    countToToken = defaultdict(list)\n",
    "    for key,value in ngramms.items():\n",
    "        countToToken[value].append(key)\n",
    "    maxCount = sorted(list(countToToken), reverse=True)\n",
    "    frequentTokens = [] \n",
    "    for c in maxCount:\n",
    "        frequentTokens += countToToken[c] \n",
    "    return frequentTokens[0:int(wordsCount*percent)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3985,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BiGramms = filterFrequentWords(2, 1, mails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Коллокации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3986,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3987,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer\n",
    "words = []\n",
    "END_OF_SENTENCE = \"<END>\"\n",
    "\n",
    "for mail in mails:\n",
    "    words.extend(word_tokenize(mail))\n",
    "    words.append(END_OF_SENTENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3988,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'1.com\", 'aeirig'),\n",
       " (\"'1HE\", \".i't\"),\n",
       " (\"'Banish\", 'Sexual'),\n",
       " (\"'H\", 'CCL'),\n",
       " (\"'Li\", 'Matou'),\n",
       " (\"'Stone\", 'Harbour'),\n",
       " (\"'bad\", 'talkers'),\n",
       " (\"'civ\", 'cas'),\n",
       " (\"'ffrjend\", 'nieei'),\n",
       " (\"'i\", 'Voz')]"
      ]
     },
     "execution_count": 3988,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(words)\n",
    "finder.nbest(bigram_measures.pmi, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4079,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def countNGramms(n, mails, indexes):\n",
    "    model = defaultdict(int)\n",
    "    \n",
    "    END_OF_SENTENCE = \"<END>\"\n",
    "    for id in indexes:\n",
    "        words = tokenize_and_stem(mails[id])\n",
    "            \n",
    "        for i in range(n-1, len(words)):\n",
    "            ngramma = \"_\".join(words[i - (n-1):i+1])\n",
    "            model[ngramma] += 1\n",
    "    countToModel = defaultdict(list)\n",
    "    for key, value in model.items():\n",
    "        countToModel[value].append(key)\n",
    "    sortedCount = sorted(countToModel.keys(),reverse=True)\n",
    "    results = []\n",
    "    for c in sortedCount[:20]:\n",
    "        results.extend(countToModel[c])\n",
    "    return results[:10]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ищу потенциальные стоп-слова - слова, которые встречаются в большом количестве документов(высокая df). Параметром max_df не пользовалась, потому что хотелось посмотреть на частотные слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4018,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stopUniGramms = filterFrequentWords(1, 0.0005, mails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4019,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "['call', 'work', 'state', 'time', 'want', 'know', 'meet', 'secretari', 'talk', 'today', 'need', 'think', 'make', 'offic', 'tomorrow', 'presid', 'issu', 'come', 'take', 'said', 'week', 'thank', 'year', 'hous', 'discuss', 'peopl', 'help', 'part', 'secur', 'report', 'sent', 'includ', 'govern', 'point', 'follow', 'support', 'hope', 'send', 'look', 'countri', 'plan', 'polici', 'made', 'clinton', 'world', 'thing', 'email', 'releas', 'depart', 'chang', 'washington', 'leader', 'give', 'morn', 'minist', 'speech', 'group', 'assist', 'told', 'statement', 'staff', 'messag']\n"
     ]
    }
   ],
   "source": [
    "print(len(stopUniGramms))\n",
    "print(stopUniGramms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4020,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2),max_df=0.1, min_df = 2,tokenizer=tokenize_and_stem, \n",
    "                             stop_words=stopwords.words(\"english\")+stopUniGramms)\n",
    "X = vectorizer.fit_transform(mails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4021,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4022,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19013"
      ]
     },
     "execution_count": 4022,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4073,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_clusters=7\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=n_clusters, random_state=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4074,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=7, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=20, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 4074,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество писем для каждого кластера. При любых экспериментах выделяется крупный кластер и несколько маленьких. \n",
    "Маленькие кластеры характеризуются редкими словами. Большой описывается более частотными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4075,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 179\n",
      "1 103\n",
      "2 61\n",
      "3 76\n",
      "4 4157\n",
      "5 634\n",
      "6 114\n"
     ]
    }
   ],
   "source": [
    "km.labels_.sort()\n",
    "curIndex = 0\n",
    "curCount = 0\n",
    "for i in range(len(km.labels_)):\n",
    "    if km.labels_[i] != curIndex:\n",
    "        print(curIndex, curCount)\n",
    "        curCount = 0\n",
    "        curIndex += 1\n",
    "\n",
    "    curCount += 1\n",
    "print(curIndex, curCount)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нашла примеры использования описания центра кластера в качестве характеристики. \n",
    "\n",
    "Темы угадываются, но если центр удален, может быть не очень хорошо. Но набор тем определить можно.\n",
    "\n",
    "Вообще кажется, что на очень частотные слова смотреть плохо, потому что кластеры могут быть вытянуты по каким-то напрвлениям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4076,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0  centroid\n",
      "Huma, Lona, Abedin, Valmoro, sheet, Lona Valmoro, Abedin Huma, Huma Abedin, check, Jake\n",
      "\n",
      "\n",
      "Cluster  1  centroid\n",
      "print, print copy, Oscar print, Oscar, copy, Huma print, Huma, print Review, please print, Review\n",
      "\n",
      "\n",
      "Cluster  2  centroid\n",
      "date, BENGHAZI, REDACTIONS, FOIA, agreement information, REDACTIONS FOIA, information REDACTIONS, Dept producer, Comm agreement, producer Select\n",
      "\n",
      "\n",
      "Cluster  3  centroid\n",
      "Room, route, Residence, conference, conference Room, Airport, floor, Residence Residence, Room floor, treaty Room\n",
      "\n",
      "\n",
      "Cluster  4  centroid\n",
      "check, schedule, confirm, list, Cheryl, Jake, travels, copy, bill, thought\n",
      "\n",
      "\n",
      "Cluster  5  centroid\n",
      "Blackberry, votes, START, women, parties, Obama, election, deal, developing, administration\n",
      "\n",
      "\n",
      "Cluster  6  centroid\n",
      "drafted, edits, comments, letter, drafted letter, Review, revised, revised drafted, Lissa, Jake\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(n_clusters):\n",
    "    print(\"Cluster \", i, \" centroid\")\n",
    "    print(\", \".join([\n",
    "            \" \".join([stemToWord[name] for name in feature_names[ind].split(\" \")])\n",
    "                for ind in order_centroids[i, :10]]))\n",
    "    print()\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделяются кластера с темами про выборы; письма для помощников Хиллари Клинтон с какими-то указаниями;про расписание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на частотные n-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4080,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0\n",
      "INFORMATION REDACTIONS, REDACTIONS FOIA, SUBJECT AGREEMENT, SENSITIVE INFORMATION, FOIA WAIVER, AGREEMENT SENSITIVE, BENGHAZI COMM, PRODUCED HOUSE, COMM SUBJECT, SELECT BENGHAZI\n",
      "\n",
      "Cluster  1\n",
      "State Dept, HOUSE SELECT, SELECT BENGHAZI, Date State, BENGHAZI COMM, SENSITIVE INFORMATION, REDACTIONS FOIA, COMM SUBJECT, SUBJECT AGREEMENT, PRODUCED HOUSE\n",
      "\n",
      "Cluster  2\n",
      "week June, meet week, behalf brother, Dougherty reporting, plan conf, Donilon told, Thank kind, kind offer, comment Washington, stars aligned\n",
      "\n",
      "Cluster  3\n",
      "advice watch, Chris hill, word advice, watch step, Morocco ground, forwarded Jake, Thanks send, reviewing edits, Thanks s000, anyone stature\n",
      "\n",
      "Cluster  4\n",
      "Secretary office, White house, state Department, units state, have been, Secretary state, Private residing, health care, conference Room, President Obama\n",
      "\n",
      "Cluster  5\n",
      "states Department, secretary office, unit states, having been, Private residents, Strategic Dialogue, Port Prince, Dialogue press, press Clips, office time\n",
      "\n",
      "Cluster  6\n",
      "United State, Start Treaty, Secretary office, missile defenses, State department, conference Room, White House, FOREIGN ministers, have been, Secretary State\n",
      "\n"
     ]
    }
   ],
   "source": [
    "curLabel = 0\n",
    "mailsLabeled = []\n",
    "curMails = []\n",
    "\n",
    "indexes = km.labels_.argsort()\n",
    "\n",
    "for labelInd in indexes:\n",
    "    label = km.labels_[labelInd]\n",
    "    if label != curLabel:\n",
    "        curLabel = label\n",
    "        mailsLabeled.append(curMails)\n",
    "        curMails = []\n",
    "        \n",
    "   \n",
    "    curMails.append(labelInd)\n",
    "mailsLabeled.append(curMails)\n",
    "for i in range(n_clusters):\n",
    "    print(\"Cluster \", i)\n",
    "    print(\", \".join([\n",
    "            \" \".join([stemToWord[v] for v in name.split(\"_\")]) for name in countNGramms(2, mails, mailsLabeled[i])]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Оценка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "1) Цель: проверить, насколько хорошо связаны слова внутри кластеров\n",
    "\n",
    "Задание: найдите лишнее слово.\n",
    "\n",
    "Предлагается 5 слов, описывающих кластер, + слово из другого кластера\n",
    "Для каждого кластера, получаем величину 1-\"Кол-во ошибок/кол-во асессоров\"\n",
    "\n",
    "UPD: Опрос показал, что лучше искать несколько лишних слов.\n",
    "\n",
    "\n",
    "\n",
    "Проблема с интрепретацией самого большого кластера. \n",
    "\n",
    "2) Цель: проверить, насколько хорошо различаются слова из разных кластеров\n",
    "\n",
    "Задание:  Даны n+1 слов из n  кластеров(при этом 2 слова из одного кластера). Найдите пару.\n",
    "\n",
    "Для каждого кластера, получаем величину 1- \"Кол-во ошибок/кол-во асессоров\"\n",
    "\n",
    "UPD: Лучше выбирать небольшое количество слов. Здесь стало показательным, что кластеризация плохая.\n",
    "\n",
    "\n",
    "3) Цель: проверить, насколько хорошо связаны слова внутри кластеров\n",
    "\n",
    "Задание: Дано описание кластера. Исключите лишние слова.\n",
    "\n",
    "\n",
    "Итог: С 3 заданием сработало лучше всего. В целом кластеризация не очень удалась. \n",
    "Очертания некоторых тем выделяются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python346",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
