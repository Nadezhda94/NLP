{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1387,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ds = pd.read_csv(\"hillary-clinton-emails/Emails.csv\", \n",
    "                 delimiter=\",\", usecols =[\"MetadataSubject\",\"ExtractedSubject\", \"ExtractedBodyText\",\"RawText\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Предобработка датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "В качестве текста исходных писем выбиралось поле ExtractedBodyText. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3816,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"H <hrod17@clintonernaii.com›\\nWednesday, September 12, 2012 11:26 PM\\nesullivanjj@state.gov'\\nFw: Fwd: more on libya\\nLibya 37 sept 12 12,docx\\nWe should get this around asap.\""
      ]
     },
     "execution_count": 3816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.ExtractedBodyText[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "На основе \"плохих\" частотных слов и структуры исходных писем выделила небольшой список стоп-слов и выражений, которые надо удалить из исходного текста перед обработкой. Например, строки с датой и пр.метаинформация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3817,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stop_words = [\"UNCLASSIFIED\", \"U.S. Department of State\"]\n",
    "stop_lines = [\"case no\", \"doc no\", \"importance\",\"cc:\", \"Cc\",\"Sent from\", \"h <hrod17@clintonemail.com>\", \n",
    "              \"sunday\", \"monday\",\"tuesday\", \"wednesday\", \"thursday\",\"friday\", \"saturday\"]\n",
    "stop_phrases = [\"@state.gov\",\"@clintonemail\",\"©state.gov\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Удаляю стоп-слова из писем. Слишком короткие письма не включаю в выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3818,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mails = []\n",
    "titles = []\n",
    "for i in range(1, len(ds.index)):\n",
    "    cur_mail = \"\"\n",
    "  \n",
    "    if type(ds.ExtractedBodyText[i]) is str and len(ds.ExtractedBodyText[i]) > 0:\n",
    "        lines = ds.ExtractedBodyText[i].split(\"\\n\")\n",
    "        cur_mail = \"\"\n",
    "        for line in lines:\n",
    "            for sl in stop_lines:\n",
    "                if line.lower().startswith(sl):\n",
    "                    line = \"\"\n",
    "                    break\n",
    "            for sp in stop_phrases:\n",
    "                if line.find(sp) > -1:\n",
    "                    line = \"\"\n",
    "                    break\n",
    "            if re.search('.docx', line) != None:\n",
    "                line = \"\"\n",
    "                \n",
    "            if len(line) != 0:\n",
    "                cur_word = line.replace(\"\\n\", \" \")\n",
    "                for sw in stop_words:\n",
    "                    cur_word = cur_word.replace(sw, \"\")\n",
    "                cur_mail += \" \" + cur_word + \".\"\n",
    "    if len(cur_mail) > 3:\n",
    "        mails.append(cur_mail)\n",
    "    if type(ds.ExtractedSubject[i]) is str and len(ds.ExtractedSubject[i]) > 3:\n",
    "        titles.append(ds.ExtractedSubject[i].replace(\"\\n\", \" \") +\". \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "source": [
    "### Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "На этом этапе удаляю слишком короткие слова(скорее всего они большой смысловой нагрузки не несут) + использую стеммер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3819,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3820,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemToWord = {}\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if len(token)>3 and re.match('[A-Za-z]+', token) != None:\n",
    "            filtered_tokens.append(token.strip(\".\"))\n",
    "    stems = []\n",
    "    tags = nltk.pos_tag(filtered_tokens)\n",
    "    for token, tag in tags:\n",
    "        stem = stemmer.stem(token)\n",
    "        if len(stem)>3 and (tag[0] == \"N\" or tag[0]==\"V\"):\n",
    "            stems.append(stem)\n",
    "            stemToWord[stem]=token\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3821,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['latest', 'syria', 'qaddafi', 'march', 'hillari']"
      ]
     },
     "execution_count": 3821,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_stem(mails[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3822,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "def filterFrequentWords(n, percent, mails):\n",
    "    vectorizer = CountVectorizer(ngram_range=(n,n),tokenizer=tokenize_and_stem, stop_words=stopwords.words(\"english\"))\n",
    "    X = vectorizer.fit_transform(mails)\n",
    "    \n",
    "    ngramms = defaultdict(int)\n",
    "    END_OF_SENTENCE = \"<END>\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    count = [len(np.nonzero(X[0,:])[1]) ]\n",
    "    wordsCount = 0\n",
    "    for i in X.nonzero()[1]:\n",
    "        ngramms[feature_names[i]] += 1\n",
    "        wordsCount+=1\n",
    "    \n",
    "    countToToken = defaultdict(list)\n",
    "    for key,value in ngramms.items():\n",
    "        countToToken[value].append(key)\n",
    "    maxCount = sorted(list(countToToken), reverse=True)\n",
    "    frequentTokens = [] \n",
    "    for c in maxCount:\n",
    "        frequentTokens += countToToken[c] \n",
    "    return frequentTokens[0:int(wordsCount*percent)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3823,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BiGramms = filterFrequentWords(2, 1, mails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Коллокации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3824,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3825,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer\n",
    "words = []\n",
    "END_OF_SENTENCE = \"<END>\"\n",
    "\n",
    "for mail in mails:\n",
    "    words.extend(word_tokenize(mail))\n",
    "    words.append(END_OF_SENTENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3826,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'1.com\", 'aeirig'),\n",
       " (\"'1HE\", \".i't\"),\n",
       " (\"'Banish\", 'Sexual'),\n",
       " (\"'H\", 'CCL'),\n",
       " (\"'Li\", 'Matou'),\n",
       " (\"'Stone\", 'Harbour'),\n",
       " (\"'bad\", 'talkers'),\n",
       " (\"'civ\", 'cas'),\n",
       " (\"'ffrjend\", 'nieei'),\n",
       " (\"'i\", 'Voz')]"
      ]
     },
     "execution_count": 3826,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(words)\n",
    "finder.nbest(bigram_measures.pmi, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3943,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def countNGramms(n, mails, indexes):\n",
    "    model = defaultdict(int)\n",
    "    \n",
    "    END_OF_SENTENCE = \"<END>\"\n",
    "    for id in indexes:\n",
    "        words = tokenize_and_stem(mails[id])\n",
    "            \n",
    "        for i in range(n-1, len(words)):\n",
    "            ngramma = \"_\".join(words[i - (n-1):i+1])\n",
    "            model[ngramma] += 1\n",
    "    countToModel = defaultdict(list)\n",
    "    for key, value in model.items():\n",
    "        countToModel[value].append(key)\n",
    "    sortedCount = sorted(countToModel.keys(),reverse=True)\n",
    "    results = []\n",
    "    for c in sortedCount[:30]:\n",
    "        print(c)\n",
    "        results.extend(countToModel[c])\n",
    "    return results[:20]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ищу потенциальные стоп-слова - слова, которые встречаются в большом количестве документов(высокая df). Параметром max_df не пользовалась, потому что хотелось посмотреть на частотные слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3828,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stopUniGramms = filterFrequentWords(1, 0.0003, mails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3829,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "['call', 'work', 'state', 'time', 'want', 'know', 'meet', 'talk', 'secretari', 'today', 'think', 'need', 'make', 'offic', 'tomorrow', 'presid', 'come', 'issu', 'thank', 'take', 'said', 'week', 'year', 'discuss', 'hous', 'peopl', 'help', 'part', 'secur', 'report', 'sent', 'includ', 'govern', 'follow', 'point', 'support', 'hope']\n"
     ]
    }
   ],
   "source": [
    "print(len(stopUniGramms))\n",
    "print(stopUniGramms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3912,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=50000,ngram_range=(1,2),tokenizer=tokenize_and_stem, \n",
    "                             stop_words=stopwords.words(\"english\")+stopUniGramms)\n",
    "X = vectorizer.fit_transform(mails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3913,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3914,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 3914,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3926,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_clusters=8\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=n_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3935,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=8, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 3935,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество писем для каждого кластера. При любых экспериментах выделяется крупный кластер и несколько маленьких. \n",
    "Маленькие кластеры характеризуются редкими словами. Большой описывается более частотными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3936,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 129\n",
      "1 116\n",
      "2 5458\n",
      "3 60\n",
      "4 127\n",
      "5 103\n",
      "6 216\n",
      "7 506\n"
     ]
    }
   ],
   "source": [
    "km.labels_.sort()\n",
    "curIndex = 0\n",
    "curCount = 0\n",
    "for i in range(len(km.labels_)):\n",
    "    if km.labels_[i] != curIndex:\n",
    "        print(curIndex, curCount)\n",
    "        curCount = 0\n",
    "        curIndex += 1\n",
    "\n",
    "    curCount += 1\n",
    "print(curIndex, curCount)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нашла примеры использования описания центра кластера в качестве характеристики. \n",
    "\n",
    "Темы угадываются, но если центр удален, может быть не очень хорошо.\n",
    "\n",
    "Вообще кажется, что на очень частотные слова смотреть плохо, потому что кластеры могут быть вытянуты по каким-то напрвлениям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3937,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0  centroid\n",
      "Department, Room, route, Residence, Department Department, conference Room, BENGHAZI, conference, REDACTIONS, REDACTIONS FOIA\n",
      "\n",
      "\n",
      "Cluster  1  centroid\n",
      "check, message, assistance, Received, travel, travel check, check message, copies, check assistance, please\n",
      "\n",
      "\n",
      "Cluster  2  centroid\n",
      "schedule, morning, confirmed, drafted, list, message, Cheryl, speech, letter, Hillary\n",
      "\n",
      "\n",
      "Cluster  3  centroid\n",
      "RELEASE, RELEASE B1,1.4, B1,1.4, RELEASE print, rshah, Gina, mailto, print, Denis, budgets\n",
      "\n",
      "\n",
      "Cluster  4  centroid\n",
      "emailed, access emailed, access, travel, please, assistance, emailed assistance, emailed addressing, assistance please, travel access\n",
      "\n",
      "\n",
      "Cluster  5  centroid\n",
      "print, print copies, copies, leverage, fine, types, Lauren, influence, Haiti, militant create\n",
      "\n",
      "\n",
      "Cluster  6  centroid\n",
      "send, Huma, Abedin, Abedin Huma, Huma Abedin, sheet, lona, copies, Jake, schedule\n",
      "\n",
      "\n",
      "Cluster  7  centroid\n",
      "vote, policy, women, plan, Clinton, Obama, parties, countries, election, leaders\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(n_clusters):\n",
    "    print(\"Cluster \", i, \" centroid\")\n",
    "    print(\", \".join([\n",
    "            \" \".join([stemToWord[name] for name in feature_names[ind].split(\" \")])\n",
    "                for ind in order_centroids[i, :10]]))\n",
    "    print()\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3944,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0  centroid\n",
      "DEPART, Room, route, Residence, DEPART DEPART, Conference Room, BENGHAZI, Conference, REDACTIONS, REDACTIONS FOIA\n",
      "\n",
      "\n",
      "Cluster  1  centroid\n",
      "checking, message, assistance, Received, travels, travels checking, checking message, copy, checking assistance, please\n",
      "\n",
      "\n",
      "Cluster  2  centroid\n",
      "schedule, morning, Confirmed, draft, list, message, Cheryl, speech, letter, Hillary\n",
      "\n",
      "\n",
      "Cluster  3  centroid\n",
      "RELEASE, RELEASE B1,1.4, B1,1.4, RELEASE print, rshah, Gina, mailto, print, Denis, budgets\n",
      "\n",
      "\n",
      "Cluster  4  centroid\n",
      "emailed, access emailed, access, travels, please, assistance, emailed assistance, emailed address, assistance please, travels access\n",
      "\n",
      "\n",
      "Cluster  5  centroid\n",
      "print, print copy, copy, leverage, fine, types, Lauren, Influence, Haiti, militancy create\n",
      "\n",
      "\n",
      "Cluster  6  centroid\n",
      "sending, Huma, Abedin, Abedin Huma, Huma Abedin, sheet, Lona, copy, Jake, schedule\n",
      "\n",
      "\n",
      "Cluster  7  centroid\n",
      "vote, Policy, women, Planning, Clinton, Obama, party, countries, elections, leaders\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(n_clusters):\n",
    "    print(\"Cluster \", i, \" centroid\")\n",
    "    print(\", \".join([\n",
    "            \" \".join([stemToWord[name] for name in feature_names[ind].split(\" \")])\n",
    "                for ind in order_centroids[i, :10]]))\n",
    "    print()\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделяются кластера с темами про выборы; письма для помощников Хиллари Клинтон с какими-то указаниями "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на частотные n-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3945,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0\n",
      "82\n",
      "81\n",
      "80\n",
      "17\n",
      "16\n",
      "12\n",
      "11\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "BENGHAZI COMM, SUBJECT AGREEMENT, PRODUCED HOUSE, DEPT PRODUCED, HOUSE SELECT, SENSITIVE INFORMATION, COMM SUBJECT, FOIA WAIVER, AGREEMENT SENSITIVE, REDACTIONS FOIA, INFORMATION REDACTIONS, SELECT BENGHAZI, STATE DEPT, Date STATE, United STATE, have been, Middle East, president Obama, WAIVER Date, White HOUSE\n",
      "\n",
      "Cluster  1\n",
      "127\n",
      "124\n",
      "123\n",
      "56\n",
      "45\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "SENSITIVE INFORMATION, INFORMATION REDACTIONS, Subject AGREEMENT, FOIA WAIVER, AGREEMENT SENSITIVE, REDACTIONS FOIA, State DEPT, Benghazi COMM, PRODUCED HOUSE, SELECT Benghazi, COMM Subject, Date State, DEPT PRODUCED, HOUSE SELECT, United State, State Department, Foreign Minister, White HOUSE, Secretary Clinton, Secretary office\n",
      "\n",
      "Cluster  2\n",
      "400\n",
      "376\n",
      "354\n",
      "328\n",
      "309\n",
      "225\n",
      "176\n",
      "163\n",
      "130\n",
      "126\n",
      "121\n",
      "120\n",
      "118\n",
      "116\n",
      "112\n",
      "108\n",
      "101\n",
      "100\n",
      "97\n",
      "96\n",
      "91\n",
      "90\n",
      "87\n",
      "85\n",
      "78\n",
      "77\n",
      "76\n",
      "70\n",
      "69\n",
      "65\n",
      "Secretary Office, White House, State DEPART, United State, have been, Secretary State, Private Residence, Health Care, Conference Room, presidents OBAMA, TIME Secretary, Office TIME, OBAMA administration, MEETING Secretary, DEPART State, Secretary Conference, Barack OBAMA, middle East, RELEASE part, Hillary Clinton\n",
      "\n",
      "Cluster  3\n",
      "2\n",
      "1\n",
      "statements aung, print review, Burma leaders, come call, list come, commitment people, know today, including talk, Nasrallah media, contribution stage, policies event, said offer, work Plan, Lewis work, development speech, weekend sent, office chairs, remembers telling, Krystal confirming, work preventive\n",
      "\n",
      "Cluster  4\n",
      "9\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "Secretary office, community center, York time, David Project, Ground Zero, United states, PHONE call, Islamic Society, states Department, office PHONE, Cordoba House, Middle East, Anti- Defamation, Huma Abedin, bestselling Books, leader company, Nation leader, Islamophobic crusaders, issues statements, time Secretary\n",
      "\n",
      "Cluster  5\n",
      "10\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "SECRETARY office, Private Residence, office time, White house, Keith Richard, State DEPART, time SECRETARY, Conference room, Navy Hill, SECRETARY Conference, National airport, Leslie Gelb, U.S.-PAKISTAN DIALOGUE, Reagan National, office MEETING, Rolling Stones, Washington Reagan, LaGuardia airport, room Floor, heroes heroic\n",
      "\n",
      "Cluster  6\n",
      "18\n",
      "17\n",
      "15\n",
      "10\n",
      "8\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "Dialogue press, Strategic Dialogue, press Clips, Port Prince, have been, Clips Strategic, Revolutionary Guards, people settlements, government Committee, settlements government, hikers were, settlements residents, security Committee, work settlements, Sanandaj officials, Investigative Fund, settlements months, INGO Partners, start school, International Development\n",
      "\n",
      "Cluster  7\n",
      "37\n",
      "25\n",
      "17\n",
      "14\n",
      "13\n",
      "12\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "United State, State DEPART, Secretary Office, Secretary State, Conference Room, FOREIGN MINISTER, Private Residence, Secretary Conference, RELEASE PART, TIME Secretary, Office TIME, have been, United NATIONAL, start Treaty, missile Defense, Middle East, White House, Office MEETING, Room Floor, Outer Office\n",
      "\n"
     ]
    }
   ],
   "source": [
    "curLabel = 0\n",
    "mailsLabeled = []\n",
    "curMails = []\n",
    "\n",
    "indexes = km.labels_.argsort()\n",
    "\n",
    "for labelInd in indexes:\n",
    "    label = km.labels_[labelInd]\n",
    "    if label != curLabel:\n",
    "        curLabel = label\n",
    "        mailsLabeled.append(curMails)\n",
    "        curMails = []\n",
    "        \n",
    "   \n",
    "    curMails.append(labelInd)\n",
    "mailsLabeled.append(curMails)\n",
    "for i in range(n_clusters):\n",
    "    print(\"Cluster \", i)\n",
    "    print(\", \".join([\n",
    "            \" \".join([stemToWord[v] for v in name.split(\"_\")]) for name in countNGramms(2, mails, mailsLabeled[i])]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Оценка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "1) Цель: проверить, насколько хорошо связаны слова внутри кластеров\n",
    "\n",
    "Задание: найдите лишнее слово.\n",
    "\n",
    "Предлагается 5 слов, описывающих кластер, + слово из другого кластера\n",
    "Для каждого кластера, получаем величину 1-\"Кол-во ошибок/кол-во асессоров\"\n",
    "\n",
    "UPD: Опрос показал, что лучше искать несколько лишних слов.\n",
    "\n",
    "\n",
    "\n",
    "Проблема с интрепретацией самого большого кластера. \n",
    "\n",
    "2) Цель: проверить, насколько хорошо различаются слова из разных кластеров\n",
    "\n",
    "Задание:  Даны n+1 слов из n  кластеров(при этом 2 слова из одного кластера). Найдите пару.\n",
    "\n",
    "Для каждого кластера, получаем величину 1- \"Кол-во ошибок/кол-во асессоров\"\n",
    "\n",
    "UPD: Лучше выбирать небольшое количество слов. Здесь стало показательным, что кластеризация плохая.\n",
    "\n",
    "\n",
    "3) Цель: проверить, насколько хорошо связаны слова внутри кластеров\n",
    "\n",
    "Задание: Дано описание кластера. Исключите лишние слова.\n",
    "\n",
    "\n",
    "Итог: С 3 заданием сработало лучше всего. В целом кластеризация не очень удалась. \n",
    "Очертания некоторых тем выделяются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Попробуем сделать кластеризацию просто по темам писем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python346",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
