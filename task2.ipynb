{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1387,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ds = pd.read_csv(\"hillary-clinton-emails/Emails.csv\", \n",
    "                 delimiter=\",\", usecols =[\"MetadataSubject\",\"ExtractedSubject\", \"ExtractedBodyText\",\"RawText\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Предобработка датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "В качестве текста исходных писем выбиралось поле ExtractedBodyText. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1385,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"H <hrod17@clintonernaii.com›\\nWednesday, September 12, 2012 11:26 PM\\nesullivanjj@state.gov'\\nFw: Fwd: more on libya\\nLibya 37 sept 12 12,docx\\nWe should get this around asap.\""
      ]
     },
     "execution_count": 1385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.ExtractedBodyText[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "На основе \"плохих\" частотных слов и структуры исходных писем выделила небольшой список стоп-слов и выражений, которые надо удалить из исходного текста перед обработкой. Например, строки с датой и пр.метаинформация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3322,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stop_words = [\"UNCLASSIFIED\", \"U.S. Department of State\"]\n",
    "stop_lines = [\"case no\", \"doc no\", \"importance\",\"cc:\", \"Cc\",\"Sent from\", \"h <hrod17@clintonemail.com>\", \n",
    "              \"sunday\", \"monday\",\"tuesday\", \"wednesday\", \"thursday\",\"friday\", \"saturday\"]\n",
    "stop_phrases = [\"@state.gov\",\"@clintonemail\",\"©state.gov\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Удаляю стоп-слова из писем. Слишком короткие письма не включаю в выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3323,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mails = []\n",
    "titles = []\n",
    "for i in range(1, len(ds.index)):\n",
    "    cur_mail = \"\"\n",
    "  \n",
    "    if type(ds.ExtractedBodyText[i]) is str and len(ds.ExtractedBodyText[i]) > 0:\n",
    "        lines = ds.ExtractedBodyText[i].split(\"\\n\")\n",
    "        cur_mail = \"\"\n",
    "        for line in lines:\n",
    "            for sl in stop_lines:\n",
    "                if line.lower().startswith(sl):\n",
    "                    line = \"\"\n",
    "                    break\n",
    "            for sp in stop_phrases:\n",
    "                if line.find(sp) > -1:\n",
    "                    line = \"\"\n",
    "                    break\n",
    "            if re.search('.docx', line) != None:\n",
    "                line = \"\"\n",
    "                \n",
    "            if len(line) != 0:\n",
    "                cur_word = line.replace(\"\\n\", \" \")\n",
    "                for sw in stop_words:\n",
    "                    cur_word = cur_word.replace(sw, \"\")\n",
    "                cur_mail += \" \" + cur_word + \".\"\n",
    "    if len(cur_mail) > 3:\n",
    "        mails.append(cur_mail)\n",
    "        if type(ds.ExtractedSubject[i]) is str and len(ds.ExtractedSubject[i]) > 0:\n",
    "            titles.append(ds.ExtractedSubject[i].replace(\"\\n\", \" \") +\". \")\n",
    "        else:\n",
    "            titles.append(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "source": [
    "### Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "На этом этапе удаляю слишком короткие слова(скорее всего они большой смысловой нагрузки не несут) + использую стеммер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3324,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3505,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemToWord = {}\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if len(token)>3 and re.match('[A-Za-z]+', token) != None:\n",
    "            filtered_tokens.append(token.strip(\".\"))\n",
    "    stems = []\n",
    "    tags = nltk.pos_tag(filtered_tokens)\n",
    "    for token, tag in tags:\n",
    "        stem = stemmer.stem(token)\n",
    "        if len(stem)>3 and (tag[0] == \"N\" or tag[0]==\"V\"):\n",
    "            stems.append(stem)\n",
    "            stemToWord[stem]=token\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3506,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['latest', 'syria', 'qaddafi', 'march', 'hillari']"
      ]
     },
     "execution_count": 3506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_stem(mails[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3507,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "def filterFrequentWords(n, percent, mails):\n",
    "    vectorizer = CountVectorizer(ngram_range=(n,n),tokenizer=tokenize_and_stem, stop_words=stopwords.words(\"english\"))\n",
    "    X = vectorizer.fit_transform(mails)\n",
    "    \n",
    "    ngramms = defaultdict(int)\n",
    "    END_OF_SENTENCE = \"<END>\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    count = [len(np.nonzero(X[0,:])[1]) ]\n",
    "    wordsCount = 0\n",
    "    for i in X.nonzero()[1]:\n",
    "        ngramms[feature_names[i]] += 1\n",
    "        wordsCount+=1\n",
    "    \n",
    "    countToToken = defaultdict(list)\n",
    "    for key,value in ngramms.items():\n",
    "        countToToken[value].append(key)\n",
    "    maxCount = sorted(list(countToToken), reverse=True)\n",
    "    frequentTokens = [] \n",
    "    for c in maxCount:\n",
    "        frequentTokens += countToToken[c] \n",
    "    return frequentTokens[0:int(wordsCount*percent)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3508,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BiGramms = filterFrequentWords(2, 1, mails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Коллокации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3509,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3510,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer\n",
    "words = []\n",
    "END_OF_SENTENCE = \"<END>\"\n",
    "\n",
    "for mail in mails:\n",
    "    words.extend(word_tokenize(mail))\n",
    "    words.append(END_OF_SENTENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3511,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'1.com\", 'aeirig'),\n",
       " (\"'1HE\", \".i't\"),\n",
       " (\"'Banish\", 'Sexual'),\n",
       " (\"'H\", 'CCL'),\n",
       " (\"'Li\", 'Matou'),\n",
       " (\"'Stone\", 'Harbour'),\n",
       " (\"'bad\", 'talkers'),\n",
       " (\"'civ\", 'cas'),\n",
       " (\"'ffrjend\", 'nieei'),\n",
       " (\"'i\", 'Voz')]"
      ]
     },
     "execution_count": 3511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(words)\n",
    "finder.nbest(bigram_measures.pmi, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3512,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countNGramms(n, mails, indexes):\n",
    "    model = defaultdict(int)\n",
    "    \n",
    "    END_OF_SENTENCE = \"<END>\"\n",
    "    for id in indexes:\n",
    "        words = tokenize_and_stem(mails[id])\n",
    "            \n",
    "        for i in range(n-1, len(words)):\n",
    "            ngramma = \"_\".join(words[i - (n-1):i+1])\n",
    "            model[ngramma] += 1\n",
    "    countToModel = defaultdict(list)\n",
    "    for key, value in model.items():\n",
    "        countToModel[value].append(key)\n",
    "    sortedCount = sorted(countToModel.keys(),reverse=True)\n",
    "    print([countToModel[c] for c in sortedCount[:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ищу потенциальные стоп-слова - слова, которые встречаются в большом количестве документов(высокая df). Параметром max_df не пользовалась, потому что хотелось посмотреть на частотные слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3513,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stopUniGramms = filterFrequentWords(1, 0.0003, mails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3514,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "['call', 'work', 'state', 'time', 'want', 'know', 'meet', 'talk', 'secretari', 'today', 'think', 'need', 'make', 'offic', 'tomorrow', 'presid', 'come', 'issu', 'thank', 'take', 'said', 'week', 'year', 'discuss', 'hous', 'peopl', 'help', 'part', 'secur', 'report', 'sent', 'includ', 'govern', 'follow', 'point', 'support', 'hope']\n"
     ]
    }
   ],
   "source": [
    "print(len(stopUniGramms))\n",
    "print(stopUniGramms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3656,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=50000,ngram_range=(1,2),tokenizer=tokenize_and_stem, \n",
    "                             stop_words=stopwords.words(\"english\")+stopUniGramms)\n",
    "X = vectorizer.fit_transform(mails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3657,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3658,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 3658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3662,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_clusters=7\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=n_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3663,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=7, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 3663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество писем для каждого кластера. При любых экспериментах выделяется крупный кластер и несколько маленьких. \n",
    "Маленькие кластеры характеризуются редкими словами. Большой описывается более частотными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3664,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 125\n",
      "1 5420\n",
      "2 79\n",
      "3 103\n",
      "4 219\n",
      "5 79\n",
      "6 690\n"
     ]
    }
   ],
   "source": [
    "km.labels_.sort()\n",
    "curIndex = 0\n",
    "curCount = 0\n",
    "for i in range(len(km.labels_)):\n",
    "    if km.labels_[i] != curIndex:\n",
    "        print(curIndex, curCount)\n",
    "        curCount = 0\n",
    "        curIndex += 1\n",
    "\n",
    "    curCount += 1\n",
    "print(curIndex, curCount)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нашла примеры использования описания центра кластера в качестве характеристики. \n",
    "Кажется, что поиски самых частотных  n-грамм не очень хорошо работают."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3666,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0  centroid\n",
      "department, room, route, residence, department department, conference room, conference, benghazi, redactions, agreement information\n",
      "\n",
      "\n",
      "Cluster  1  centroid\n",
      "released, email, check, schedule, message, morning, list, hillary, looks, copy\n",
      "\n",
      "\n",
      "Cluster  2  centroid\n",
      "prepare, signed, prepare signed, letter, thing, review, morning, prepare statement, statement, monday\n",
      "\n",
      "\n",
      "Cluster  3  centroid\n",
      "print, print copy, copy, leverage, fine, types, lauren, influence, haiti, militancy create\n",
      "\n",
      "\n",
      "Cluster  4  centroid\n",
      "sends, draft, done, speech, edited, sheet, review, revisions, letter, copy\n",
      "\n",
      "\n",
      "Cluster  5  centroid\n",
      "confirm, oscar, morning, confirm fact, confirm morning, sheet, fact, check, confirm connection, espinosa\n",
      "\n",
      "\n",
      "Cluster  6  centroid\n",
      "huma, assistant, cheryl, lona, abedin, clinton, valmoro, mills, policy, valmoro assistant\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(n_clusters):\n",
    "    print(\"Cluster \", i, \" centroid\")\n",
    "    print(\", \".join([\n",
    "            \" \".join([stemToWord[name] for name in feature_names[ind].split(\" \")])\n",
    "                for ind in order_centroids[i, :10]]))\n",
    "    print()\n",
    "    print() \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Оценка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "1) Цель: проверить, насколько хорошо связаны слова внутри кластеров\n",
    "\n",
    "Задание: найдите лишнее слово.\n",
    "\n",
    "Предлагается 5 слов, описывающих кластер, + слово из другого кластера\n",
    "Для каждого кластера, получаем величину 1-\"Кол-во ошибок/кол-во асессоров\"\n",
    "\n",
    "UPD: Опрос показал, что лучше искать несколько лишних слов.\n",
    "\n",
    "\n",
    "\n",
    "Проблема с интрепретацией самого большого кластера. \n",
    "\n",
    "2) Цель: проверить, насколько хорошо различаются слова из разных кластеров\n",
    "\n",
    "Задание:  Даны n+1 слов из n  кластеров(при этом 2 слова из одного кластера). Найдите пару.\n",
    "\n",
    "Для каждого кластера, получаем величину 1- \"Кол-во ошибок/кол-во асессоров\"\n",
    "\n",
    "UPD: Лучше выбирать небольшое количество слов. Здесь стало показательным, что кластеризация плохая.\n",
    "\n",
    "\n",
    "3) Цель: проверить, насколько хорошо связаны слова внутри кластеров\n",
    "\n",
    "Задание: Дано описание кластера. Исключите лишние слова.\n",
    "\n",
    "\n",
    "Итог: С 3 заданием сработало лучше всего. В целом кластеризация не очень удалась. \n",
    "Очертания тем выделяются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python346",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
