{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Семинар 3. N-gram language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучать языковую модель будем на открытом корпусе русского языка.<br/>\n",
    "Работать с ним удобно с помощью модуля opencorpora-tools, который предоставляет NLTK-подобный интерфейс к нему."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencorpora-tools in /Users/hope/.pyenv/versions/3.4.6/envs/py346/lib/python3.4/site-packages\r\n",
      "Requirement already satisfied: lxml in /Users/hope/.pyenv/versions/3.4.6/envs/py346/lib/python3.4/site-packages (from opencorpora-tools)\r\n"
     ]
    }
   ],
   "source": [
    "# Установим opencorpora-tools\n",
    "# Если у вас anaconda, лучше используйте команду conda вместо pip.\n",
    "!pip install opencorpora-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/hope/.pyenv/versions/py346/bin/opencorpora\", line 3, in <module>\n",
      "    from opencorpora import cli\n",
      "  File \"/Users/hope/.pyenv/versions/3.4.6/envs/py346/lib/python3.4/site-packages/opencorpora/__init__.py\", line 4, in <module>\n",
      "    from .reader_lxml import load\n",
      "  File \"/Users/hope/.pyenv/versions/3.4.6/envs/py346/lib/python3.4/site-packages/opencorpora/reader_lxml.py\", line 8, in <module>\n",
      "    from lxml import etree\n",
      "ImportError: dlopen(/Users/hope/.pyenv/versions/3.4.6/envs/py346/lib/python3.4/site-packages/lxml/etree.so, 2): Library not loaded: libxml2.2.dylib\n",
      "  Referenced from: /Users/hope/.pyenv/versions/3.4.6/envs/py346/lib/python3.4/site-packages/lxml/etree.so\n",
      "  Reason: Incompatible library version: etree.so requires version 12.0.0 or later, but libxml2.2.dylib provides version 10.0.0\n"
     ]
    }
   ],
   "source": [
    "# Скачаем корпус\n",
    "!opencorpora download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Языковые модели на N-граммах.ipynb\r\n",
      "annot.opcorpora.xml\r\n",
      "annot.opcorpora.xml.~\r\n",
      "Летучка.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls # Файл с корпусом должен был появиться в текущей папке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import opencorpora\n",
    "corpus = opencorpora.CorpusReader('annot.opcorpora.xml')\n",
    "docs = corpus.catalog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', '\"Частный корреспондент\"'),\n",
       " ('2', '00021 Школа злословия'),\n",
       " ('3', '00022 Последнее восстание в Сеуле'),\n",
       " ('4', '00023 За кота - ответишь!'),\n",
       " ('5', '00024 Быстротечный кинороман'),\n",
       " ('6', '00014 Холодная ванна возвращает силы'),\n",
       " ('7', '00031 Рецессия в Латвии и Эстонии'),\n",
       " ('8', 'Википедия'),\n",
       " ('9', '06037 100 дней Обамы: iТоги'),\n",
       " ('10', '01961 100 миллиардов может и не хватить')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['«', 'Школа', 'злословия', '»', 'учит', 'прикусить', 'язык']\n",
      "['Сохранится', 'ли', 'градус', 'дискуссии', 'в', 'новом', 'сезоне', '?']\n",
      "['Великолепная', '«', 'Школа', 'злословия', '»', 'вернулась', 'в', 'эфир', 'после', 'летних', 'каникул', 'в', 'новом', 'формате', '.']\n",
      "['В', 'истории', 'программы', 'это', 'уже', 'не', 'первый', '«', 'ребрендинг', '»', '.']\n",
      "['Сейчас', 'с', 'трудом', 'можно', 'припомнить', ',', 'что', 'начиналась', '«', 'Школа', '…', '»', 'на', 'канале', '«', 'Культура', '»', 'как', 'стандартное', 'ток-шоу', ',', 'которое', 'отличалось', 'от', 'других', '«', 'кухонными', '»', 'обсуждениями', 'гостя', ',', 'что', 'называется', '–', '«', 'за', 'глаза', '»', ',', 'и', 'неожиданными', 'персонами', 'в', 'качестве', 'ведущих', '.']\n"
     ]
    }
   ],
   "source": [
    "# Выведем несколько предложений из корпуса.\n",
    "for id, name in docs[:2]:\n",
    "    # Можно использовать corpus.tagged_sents(id), чтобы отсеять пунктуаторы.\n",
    "    sents = corpus.sents(id)\n",
    "    for words in sents[:5]:\n",
    "        \n",
    "        print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(docs, n):\n",
    "    ngramms = {}\n",
    "    END_OF_SENTENCE = \"<END>\"\n",
    "    for id, name in docs:\n",
    "        sents = corpus.sents(id)\n",
    "        for words in sents:\n",
    "            words.append(END_OF_SENTENCE)\n",
    "            \n",
    "            for i in range(n, len(words)):\n",
    "                ngramma = \"_\".join(words[i - (n-1):i])\n",
    "                if ngramms.get(ngramma) == None:\n",
    "                    ngramms[ngramma] = {}\n",
    "                if ngramms[ngramma].get(words[i]) == None:\n",
    "                    ngramms[ngramma][words[i]] = 0\n",
    "                ngramms[ngramma][words[i]] += 1\n",
    "    return ngramms\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите функцию train_model(docs, n), которая возвращает обученную n-граммную модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель предлагается сделать в виде отображения: <br/>\n",
    "(n-1)-грамма => {следующее слово => вероятность всей n-грамы}<br/>\n",
    "или<br/>\n",
    "(n-1)-грамма => {следующее слово => количество соответствующих n-грамм в корпусе}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Введите специальное слово, обозначающее конец предложения.<br/>Добавляйте его в n-граммы, которые находятся в конце предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def train_model(docs, n):\n",
    "    # model = {(word_1, ..., word_n-1): {word_n: 0,2, word_n: 0,1}}\n",
    "    model = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    END_OF_SENTENCE = \"<END>\"\n",
    "    for id, name in docs:\n",
    "        sents = corpus.sents(id)\n",
    "        for words in sents:\n",
    "            words.append(END_OF_SENTENCE)\n",
    "            \n",
    "            for i in range(n, len(words)):\n",
    "                ngramma = \"_\".join(words[i - (n-1):i])\n",
    "                model[ngramma][words[i]] += 1\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = train_model(docs, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите функцию get_sentence_probability(model, tokens), которая будет возвращать вероятность предложения в n-граммной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_probability(model, tokens):\n",
    "    n = 1\n",
    "    keys = list(model)\n",
    "    if len(keys) > 1:\n",
    "        n += 1 + keys[0].count(\"_\")\n",
    "    if len(tokens) < n:\n",
    "        return 0.0\n",
    "    prob = 1.0\n",
    "    for i in range(n-1, len(tokens)):\n",
    "        ngramma = \"_\".join(words[i - (n-1):i])\n",
    "        sNGramm = sum(model[ngramma].values())\n",
    "        curProb = 0\n",
    "        if sNGramm != 0:\n",
    "            curProb = model[ngramma][words[i]]/sNGramm\n",
    "        prob *= curProb\n",
    "    return prob\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.976540128682568e-22"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_probability(model, \"Статья 5. Независимость судей арбитражных судов\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите функцию generate_sentence(model, max_sentence_length), которая будет генерировать случайные предложения из распределения, описываемого моделью model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_next_word(model, prevNWords):  \n",
    "    #print(model[prevNWords])\n",
    "    nextPossibleWords = list(model[prevNWords].keys())\n",
    "    probs = list(model[prevNWords].values())\n",
    "    probsCount = len(nextPossibleWords)\n",
    "    \n",
    "    choice = random.uniform(0.0, sum(probs))\n",
    "    curSum = 0.0\n",
    "    for i in range(probsCount):\n",
    "        curSum += probs[i]\n",
    "        if choice <= curSum:\n",
    "            return nextPossibleWords[i]\n",
    "\n",
    "def generate_sentence(model, max_sentence_length):\n",
    "    n = 1\n",
    "    keys = list(model)\n",
    "    if len(keys) > 1:\n",
    "        n += 1 + keys[0].count(\"_\")\n",
    "    \n",
    "    startWord = keys[random.randint(0, len(keys)-1)]\n",
    "    sent = startWord.split(\"_\")\n",
    "    prevNWord = startWord\n",
    "\n",
    "    for i in range(n-1, max_sentence_length):\n",
    "\n",
    "        nextWord = generate_next_word(model, prevNWord)\n",
    "        if nextWord == \"<END>\":\n",
    "            break\n",
    "\n",
    "        sent.append(nextWord)\n",
    "        if n != 1:\n",
    "            prevNWord = \"_\".join(sent[-n+1:])\n",
    "\n",
    "    return \" \".join(sent)   \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрите, какие предложения генерирует unigram-модель, bigram-модель, trigram-модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1 = train_model(docs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = train_model(docs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3 = train_model(docs, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " разделением сослагательном оформит пополнении сдать мероприятие (\n",
      "санатория , а не берет книгу Нормана\n",
      "ситуацию решает он , что вопросы обеспечения\n"
     ]
    }
   ],
   "source": [
    "print(generate_sentence(model1, 7))\n",
    "print(generate_sentence(model2, 7))\n",
    "print(generate_sentence(model3, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите функцию suggest, которая будет предсказывать следующее слово для слов tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def suggest(model, tokens):\n",
    "    n = 1\n",
    "    keys = list(model)\n",
    "    if len(keys) > 1:\n",
    "        n += 1 + keys[0].count(\"_\")\n",
    "\n",
    "    lastNTokens = \"_\".join(tokens[-n+1:])\n",
    "    countToToken = {value:key for key, value in model[lastNTokens].items()}\n",
    "    maxCount = sorted(list(countToToken), reverse=True)[0]\n",
    "    return countToToken[maxCount]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'дискуссии'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggest(model3, \"Сохранится ли градус\".split( \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перепишите функцию train_model так, чтобы она использовала сглаживание вероятностей n-грамм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(docs, n):\n",
    "    # model = {(word_1, ..., word_n-1): {word_n: 0,2, word_n: 0,1}}\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 1))\n",
    "    \n",
    "    END_OF_SENTENCE = \"<END>\"\n",
    "    for id, name in docs:\n",
    "        sents = corpus.sents(id)\n",
    "        for words in sents:\n",
    "            words.append(END_OF_SENTENCE)\n",
    "            \n",
    "            for i in range(n, len(words)):\n",
    "                ngramma = \"_\".join(words[i - (n-1):i])\n",
    "                model[ngramma][words[i]] += 1\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python346",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
